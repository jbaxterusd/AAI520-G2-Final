{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Project\n",
    "#AAI-520 \n",
    "\n",
    "#Callum Lamb - Team Leader\n",
    "#Deepti Pamula\n",
    "#Jack Baxter\n",
    "#Jasper Dolar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 0\n",
    "\n",
    "#Project Description — What this project does\n",
    "\n",
    "#This notebook builds an autonomous investment research \n",
    "#agent that researches a stock end-to-end.\n",
    "#It dynamically routes user requests to the right tools \n",
    "#(web search, price lookup, daily trend, and news analysis), \n",
    "#runs a prompt-chained NLP pipeline on recent news \n",
    "#(preprocess → VADER sentiment → Hugging Face NER → summary), \n",
    "#and then self-evaluates the write-up with a tiny LLM grader.\n",
    "#The agent keeps memory across turns via LangGraph’s MemorySaver \n",
    "#(thread_id), so preferences like a watchlist persist within \n",
    "#a session. \n",
    "# \n",
    "#Demos show Q&A, a compact news brief, and a sentiment\n",
    "#plot—covering the rubric’s Agent Functions and Workflow Patterns.\n",
    "\n",
    "#Project Requirements / Rubric Map\n",
    "#--------------------------\n",
    "#Agent Functions (33.8%)\n",
    "#- Dynamic tool use & planning \n",
    "#  Cells 7, 8, 15\n",
    "\n",
    "#- Self-reflection / evaluation \n",
    "#  Cell 11 (optimize_news -> score 0–4)\n",
    "\n",
    "#- Learns across runs (memory) \n",
    "#  Cells 8–9 (MemorySaver + thread_id)\n",
    "#\n",
    "\n",
    "#--------------------------\n",
    "#Workflow Patterns (33.8%)\n",
    "#1) Prompt Chaining \n",
    "#   (ingest→preprocess→classify→extract→summarize)\n",
    "#   Cells 10–11, 13\n",
    "\n",
    "#2) Routing (send to the right tool)\n",
    "#   Cells 7–8, 15 (with demos in 9, 14–15)\n",
    "\n",
    "#3) Evaluator–Optimizer\n",
    "#   Cell 11 (score); comments describe refinement logic\n",
    "#   - see optional Cell 16a for auto-refinement demo\n",
    "\n",
    "#Code (32.4%)\n",
    "#Submitted in PDF\n",
    "#Relevant inline comments included\n",
    "#GitHub repository link included\n",
    "\n",
    "#------------------------\n",
    "# Technology Requirements - with APIs\n",
    "# - Yahoo Finance (yfinance) \n",
    "#   Cells 7, 10, 13\n",
    "\n",
    "# - Tavily (web search) \n",
    "#   Cell 5\n",
    "# - Finnhub (company news) \n",
    "#   Cells 7, 13\n",
    "# - Alpha Vantage (daily series) \n",
    "#   Cell 7\n",
    "#\n",
    "\n",
    "#------------------------\n",
    "# Deliverable Evidence\n",
    "# - Design/Workflows/Capabilities \n",
    "#   Inline comments (Cells 7–8, 10–11, 15)\n",
    "\n",
    "# - Demos & outputs \n",
    "#   Cells 9, 12, 14, 15\n",
    "\n",
    "# - Evaluation score printed \n",
    "#   Cells 12, 13, 15\n",
    "# - Viz (optional polish) \n",
    "#   Cell 17\n",
    "\n",
    "#-----------------------\n",
    "# Run order:\n",
    "# 0 → 1 → 2 → 3 → (3b if first setup) → 4 → 5 → 7 \n",
    "# → 8 → 9 → 10 → 11 → 12 → 13 → (16a optional) \n",
    "# → 14 → 15 → 17\n",
    "\n",
    "# Optional: \n",
    "# Cell 6 - redundant (env-set keys in Cell 5)\n",
    "# Cells 6/16a - optional demos \n",
    "# Cell 12b - debug cell \n",
    "\n",
    "# Note: If Cell 3 is used for env fixes, run it once after Cell 2 then restart kernel.\n",
    "\n",
    "#-----------------------\n",
    "#Citation:\n",
    "#Code corrections and debugging guidance were adapted from\n",
    "#\"Zero to Mastery\" AI Course and OpenAI consultation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 1\n",
    "#Final Project Script  \n",
    "#Agentic AI for financial analysis \n",
    "#MODEL/CODE INSPO: https://python.langchain.com/docs/tutorials/agents/\n",
    "#xAI Integration: https://x.ai/api#capabilities\n",
    "#Agent Params/Structure: https://langchain-ai.github.io/langgraph/reference/agents/, \n",
    "#https://api.python.langchain.com/en/latest/messages/langchain_core.messages.system.SystemMessage.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Old Installs below\\n# Core LangChain / LangGraph packages for building the ReAct-style agent\\n%pip install -U langgraph langchain-tavily langgraph-checkpoint-sqlite\\n\\n# Hugging Face integration for text-generation and NER pipelines\\n%pip install langchain-huggingface\\n\\n# xAI (Grok-2) connector used as the LLM back-end in your agent\\n%pip install -qU langchain-xai\\n\\n# Finance and NLP utilities\\n%pip install yfinance       # financial news & price data\\n%pip install nltk           # tokenization, stopwords, sentiment (VADER)\\n%pip install transformers   # Hugging Face transformer models\\n%pip install torch          # deep-learning backend for transformers\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cell 2\n",
    "# ============================================================\n",
    "# Environment & Package Installation (handled in Cell 3)\n",
    "# ============================================================\n",
    "# We install everything once from the current interpreter in Cell 3 to avoid env mismatch.\n",
    "\n",
    "'''Old Installs below\n",
    "# Core LangChain / LangGraph packages for building the ReAct-style agent\n",
    "%pip install -U langgraph langchain-tavily langgraph-checkpoint-sqlite\n",
    "\n",
    "# Hugging Face integration for text-generation and NER pipelines\n",
    "%pip install langchain-huggingface\n",
    "\n",
    "# xAI (Grok-2) connector used as the LLM back-end in your agent\n",
    "%pip install -qU langchain-xai\n",
    "\n",
    "# Finance and NLP utilities\n",
    "%pip install yfinance       # financial news & price data\n",
    "%pip install nltk           # tokenization, stopwords, sentiment (VADER)\n",
    "%pip install transformers   # Hugging Face transformer models\n",
    "%pip install torch          # deep-learning backend for transformers\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Python: /opt/anaconda3/envs/aai520-nlp/bin/python\n",
      "Requirement already satisfied: langgraph>=0.6.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (0.6.10)\n",
      "Requirement already satisfied: langgraph-prebuilt>=0.6.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (0.6.4)\n",
      "Requirement already satisfied: langgraph-checkpoint-sqlite>=2.0.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (2.0.11)\n",
      "Requirement already satisfied: langchain-core>=0.3.70 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (0.3.79)\n",
      "Requirement already satisfied: langchain-tavily in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (0.2.12)\n",
      "Requirement already satisfied: langchain-huggingface in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (0.3.1)\n",
      "Requirement already satisfied: langchain-xai in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (0.2.5)\n",
      "Requirement already satisfied: yfinance in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (0.2.66)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (4.57.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langgraph>=0.6.0) (2.1.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langgraph>=0.6.0) (0.2.9)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langgraph>=0.6.0) (2.11.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langgraph>=0.6.0) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph>=0.6.0) (1.11.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph>=0.6.0) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph>=0.6.0) (3.11.3)\n",
      "Requirement already satisfied: aiosqlite>=0.20 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langgraph-checkpoint-sqlite>=2.0.0) (0.21.0)\n",
      "Requirement already satisfied: sqlite-vec>=0.1.6 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langgraph-checkpoint-sqlite>=2.0.0) (0.1.6)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langchain-core>=0.3.70) (0.4.31)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langchain-core>=0.3.70) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langchain-core>=0.3.70) (1.33)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langchain-core>=0.3.70) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langchain-core>=0.3.70) (4.15.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langchain-core>=0.3.70) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.3.70) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.70) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.70) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.70) (0.25.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph>=0.6.0) (4.11.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph>=0.6.0) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph>=0.6.0) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph>=0.6.0) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph>=0.6.0) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from pydantic>=2.7.4->langgraph>=0.6.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from pydantic>=2.7.4->langgraph>=0.6.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from pydantic>=2.7.4->langgraph>=0.6.0) (0.4.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langchain-tavily) (3.12.15)\n",
      "Requirement already satisfied: langchain<2.0.0,>=0.3.20 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langchain-tavily) (0.3.27)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.20.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langchain<2.0.0,>=0.3.20->langchain-tavily) (0.3.11)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langchain<2.0.0,>=0.3.20->langchain-tavily) (2.0.43)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.70) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.70) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain<2.0.0,>=0.3.20->langchain-tavily) (3.2.4)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langchain-huggingface) (0.22.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.4 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langchain-huggingface) (0.35.0)\n",
      "Requirement already satisfied: langchain-openai<0.4,>=0.3.28 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langchain-xai) (0.3.35)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langchain-openai<0.4,>=0.3.28->langchain-xai) (2.3.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from langchain-openai<0.4,>=0.3.28->langchain-xai) (0.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from openai<3.0.0,>=1.104.2->langchain-openai<0.4,>=0.3.28->langchain-xai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from openai<3.0.0,>=1.104.2->langchain-openai<0.4,>=0.3.28->langchain-xai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from openai<3.0.0,>=1.104.2->langchain-openai<0.4,>=0.3.28->langchain-xai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from openai<3.0.0,>=1.104.2->langchain-openai<0.4,>=0.3.28->langchain-xai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai<0.4,>=0.3.28->langchain-xai) (2025.9.18)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from yfinance) (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from yfinance) (1.26.4)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from yfinance) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from yfinance) (4.4.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from yfinance) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from yfinance) (3.18.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from yfinance) (4.14.2)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from yfinance) (0.13.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from yfinance) (6.32.1)\n",
      "Requirement already satisfied: websockets>=13.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.10)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8)\n",
      "Requirement already satisfied: cffi>=1.12.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ langgraph imports OK\n"
     ]
    }
   ],
   "source": [
    "#Cell 3\n",
    "import sys, subprocess, importlib\n",
    "print(\"Using Python:\", sys.executable)\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\",\n",
    "    \"langgraph>=0.6.0\",\n",
    "    \"langgraph-prebuilt>=0.6.0\",\n",
    "    \"langgraph-checkpoint-sqlite>=2.0.0\",\n",
    "    \"langchain-core>=0.3.70\",\n",
    "    \"langchain-tavily\",\n",
    "    \"langchain-huggingface\",\n",
    "    \"langchain-xai\",\n",
    "    \"yfinance\",\n",
    "    \"nltk\",\n",
    "    \"transformers\",\n",
    "    \"torch\"\n",
    "])\n",
    "\n",
    "for m in [\"langgraph\", \"langgraph.prebuilt\", \"langgraph.checkpoint.memory\"]:\n",
    "    importlib.import_module(m)\n",
    "print(\"✅ langgraph imports OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: finnhub-python in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (2.4.25)\n",
      "Requirement already satisfied: alpha_vantage in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: requests>=2.22.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from finnhub-python) (2.32.5)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from alpha_vantage) (3.12.15)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from requests>=2.22.0->finnhub-python) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from requests>=2.22.0->finnhub-python) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from requests>=2.22.0->finnhub-python) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from requests>=2.22.0->finnhub-python) (2025.10.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from aiohttp->alpha_vantage) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from aiohttp->alpha_vantage) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from aiohttp->alpha_vantage) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from aiohttp->alpha_vantage) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from aiohttp->alpha_vantage) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from aiohttp->alpha_vantage) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from aiohttp->alpha_vantage) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /opt/anaconda3/envs/aai520-nlp/lib/python3.11/site-packages (from aiosignal>=1.4.0->aiohttp->alpha_vantage) (4.15.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cell 3b — add vendor SDKs used by our tools (Finnhub + Alpha Vantage)\n",
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"finnhub-python\", \"alpha_vantage\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 4\n",
    "#import packages\n",
    "\n",
    "#agent memory and orchestration\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "#LLM init (xAI Grok-2 via LangChain)\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "#web search tool (Tavily)\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "#utilities / Data/ Finance\n",
    "from datetime import datetime, timedelta \n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "#quiet some library noise\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aai520-nlp/bin/python\n"
     ]
    }
   ],
   "source": [
    "#Cell 4b\n",
    "#quick check\n",
    "import sys\n",
    "print(sys.executable) #check if the same interpreter used in cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 Keys loaded (hard-coded).\n",
      "🔎 Tavily tool ready: True\n",
      "🤖 Grok-2 ready: True\n",
      "🧭 LangSmith tracing: true | project: aai520-final project\n",
      "📦 Finnhub key set: True\n",
      "📦 Alpha Vantage key set: True\n",
      "Tavily results: 6\n",
      "Grok says: Hello class project.\n"
     ]
    }
   ],
   "source": [
    "#Cell 5\n",
    "\n",
    "#DO NOT SHARE THESE KEYS PUBLICLY OR POST IN PUBLIC PLATFORMS\n",
    "#Jasper's API Keys:\n",
    "#Tavily - **REDACTED**\n",
    "#Grok - **REDACTED**\n",
    "#Langsmith - **REDACTED**\n",
    "\n",
    "\n",
    "#THESE KEYS ARE IDEALLY TO BE ENCRYPTED, BUT FOR THIS PROJECT\n",
    "#IT's OKAY TO HARD-CODE\n",
    "\n",
    "# --- Hard-coded keys (CLASS PROJECT ONLY) ---\n",
    "TAVILY_API_KEY     = \"**API REDACTED**\"\n",
    "XAI_API_KEY        = \"**API REDACTED**\"\n",
    "LANGSMITH_API_KEY  = \"**API REDACTED**\"     # Personal Access Token\n",
    "LANGSMITH_PROJECT  = \"**API REDACTED**\"            # optional, for grouping runs\n",
    "FINNHUB_API_KEY    = \"**API REDACTED**\"\n",
    "ALPHAVANTAGE_API_KEY = \"**API REDACTED**\"\n",
    "ENABLE_TRACING     = True                      # set False to disable\n",
    "\n",
    "# --- Make libraries that expect env vars happy ---\n",
    "import os\n",
    "os.environ[\"TAVILY_API_KEY\"]        = TAVILY_API_KEY\n",
    "os.environ[\"XAI_API_KEY\"]           = XAI_API_KEY\n",
    "os.environ[\"LANGSMITH_API_KEY\"]     = LANGSMITH_API_KEY\n",
    "os.environ[\"LANGSMITH_PROJECT\"]     = LANGSMITH_PROJECT\n",
    "os.environ[\"LANGSMITH_TRACING\"]     = \"true\" if ENABLE_TRACING else \"false\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]  = os.environ[\"LANGSMITH_TRACING\"]  # legacy flag\n",
    "os.environ[\"FINNHUB_API_KEY\"]       = FINNHUB_API_KEY\n",
    "os.environ[\"ALPHAVANTAGE_API_KEY\"]  = ALPHAVANTAGE_API_KEY\n",
    "\n",
    "# ---- Init Tavily + Grok-2 ----\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "search = TavilySearch(max_results=6)                       # web search tool\n",
    "llm    = init_chat_model(\"grok-2\", model_provider=\"xai\")  # or \"grok-2-mini\"\n",
    "\n",
    "print(\"🔑 Keys loaded (hard-coded).\")\n",
    "print(\"🔎 Tavily tool ready:\", isinstance(search, TavilySearch))\n",
    "print(\"🤖 Grok-2 ready:\", llm is not None)\n",
    "print(\"🧭 LangSmith tracing:\", os.getenv(\"LANGSMITH_TRACING\"),\n",
    "      \"| project:\", os.getenv(\"LANGSMITH_PROJECT\"))\n",
    "print(\"📦 Finnhub key set:\", bool(os.getenv(\"FINNHUB_API_KEY\")))\n",
    "print(\"📦 Alpha Vantage key set:\", bool(os.getenv(\"ALPHAVANTAGE_API_KEY\")))\n",
    "\n",
    "# --- Quick checks ---\n",
    "try:\n",
    "    r = search.invoke({\"query\": \"site:investor.apple.com earnings presentation\"})\n",
    "    print(\"Tavily results:\", len(r.get(\"results\", [])))\n",
    "except Exception as e:\n",
    "    print(\"Tavily error:\", e)\n",
    "\n",
    "try:\n",
    "    resp = llm.invoke(\"Reply with exactly three words: hello class project.\")\n",
    "    print(\"Grok says:\", resp.content)\n",
    "except Exception as e:\n",
    "    print(\"Grok error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔐 Key status:\n",
      "  LangSmith: lsv2…b47c | tracing: true\n",
      "  Tavily   : tvly…69lR\n",
      "  xAI/Grok : xai-…SUvV\n",
      "  Finnhub  : set\n",
      "  AlphaVant: set\n"
     ]
    }
   ],
   "source": [
    "#Cell 6\n",
    "'''\n",
    "Jack's code: modified cuz Jasper's API keys are hard-coded\n",
    "os.environ['LANGSMITH_TRACING'] = 'true'\n",
    "os.environ['LANGSMITH_API_KEY'] = getpass.getpass('Enter the API key for LangSmith: ')\n",
    "os.environ['TAVILY_API_KEY'] = getpass.getpass('Enter the API key for Tavily: ')\n",
    "os.environ['XAI_API_KEY'] = getpass.getpass('Enter API key for xAI: ')\n",
    "os.environ['FINNHUB_API_KEY'] = getpass.getpass('Enter API key for FinnHub: ')\n",
    "os.environ['ALPHAVANTAGE_API_KEY' ] = getpass.getpass('Enter API key for Alpha Vantage: ')\n",
    "'''\n",
    "#============================================================\n",
    "#Keys (non-destructive): only prompt if missing\n",
    "#============================================================\n",
    "import os, getpass\n",
    "\n",
    "ENABLE_TRACING_DEFAULT = True  # flip to False if you want tracing off by default\n",
    "\n",
    "def _mask(v: str) -> str:\n",
    "    if not v: return \"MISSING\"\n",
    "    return f\"{v[:4]}…{v[-4:]}\" if len(v) > 8 else \"********\"\n",
    "\n",
    "def prompt_if_missing(env_name: str, prompt_label: str, required: bool = False):\n",
    "    \"\"\"If env var is missing, securely prompt for it. Return final value.\"\"\"\n",
    "    val = os.getenv(env_name, \"\")\n",
    "    if not val:\n",
    "        try:\n",
    "            entered = getpass.getpass(f\"{prompt_label} (press Enter to skip): \").strip()\n",
    "        except Exception:\n",
    "            entered = \"\"\n",
    "        if entered:\n",
    "            os.environ[env_name] = entered\n",
    "            val = entered\n",
    "        elif required:\n",
    "            raise RuntimeError(f\"{env_name} is required but not provided.\")\n",
    "    return val\n",
    "\n",
    "# --- Required for your agent setup ---\n",
    "ls_key = prompt_if_missing(\"LANGSMITH_API_KEY\", \"Enter LangSmith API key\")\n",
    "tv_key = prompt_if_missing(\"TAVILY_API_KEY\", \"Enter Tavily API key\")\n",
    "xai_key = prompt_if_missing(\"XAI_API_KEY\", \"Enter xAI (Grok) API key\")\n",
    "\n",
    "# --- Optional providers (only if you actually use them later) ---\n",
    "fh_key = prompt_if_missing(\"FINNHUB_API_KEY\", \"Enter Finnhub API key\")        # optional\n",
    "av_key = prompt_if_missing(\"ALPHAVANTAGE_API_KEY\", \"Enter Alpha Vantage API key\")  # optional\n",
    "\n",
    "# LangSmith tracing: enable only if key present (or default flag True)\n",
    "if os.getenv(\"LANGSMITH_API_KEY\") and ENABLE_TRACING_DEFAULT:\n",
    "    os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"   # legacy flag some libs still read\n",
    "else:\n",
    "    os.environ[\"LANGSMITH_TRACING\"] = \"false\"\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "\n",
    "print(\"🔐 Key status:\")\n",
    "print(\"  LangSmith:\", _mask(os.getenv(\"LANGSMITH_API_KEY\")), \"| tracing:\", os.getenv(\"LANGSMITH_TRACING\"))\n",
    "print(\"  Tavily   :\", _mask(os.getenv(\"TAVILY_API_KEY\")))\n",
    "print(\"  xAI/Grok :\", _mask(os.getenv(\"XAI_API_KEY\")))\n",
    "print(\"  Finnhub  :\", \"set\" if os.getenv(\"FINNHUB_API_KEY\") else \"skipped\")\n",
    "print(\"  AlphaVant:\", \"set\" if os.getenv(\"ALPHAVANTAGE_API_KEY\") else \"skipped\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧰 Tools ready: ['get_latest_price', 'get_daily_series', 'get_recent_news', 'analyze_news_report']\n"
     ]
    }
   ],
   "source": [
    "#Cell 7\n",
    "#============================================================\n",
    "#Tools for the agent (price, daily series, news, analysis)\n",
    "#goal: small + reliable; prefer vendor APIs, fallback to yfinance\n",
    "# ============================================================\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from datetime import datetime, timedelta\n",
    "import os, yfinance as yf\n",
    "\n",
    "# optional vendor clients — we’ll use them if keys/libs are present\n",
    "_finnhub_client = None\n",
    "try:\n",
    "    import finnhub\n",
    "    if os.getenv(\"FINNHUB_API_KEY\"):\n",
    "        _finnhub_client = finnhub.Client(api_key=os.getenv(\"FINNHUB_API_KEY\"))\n",
    "except Exception:\n",
    "    _finnhub_client = None\n",
    "\n",
    "_av_ok = False\n",
    "try:\n",
    "    from alpha_vantage.timeseries import TimeSeries\n",
    "    _av_ok = bool(os.getenv(\"ALPHAVANTAGE_API_KEY\"))\n",
    "except Exception:\n",
    "    _av_ok = False\n",
    "\n",
    "\n",
    "# ---------- PRICE ----------\n",
    "@tool\n",
    "def get_latest_price(symbol: str) -> str:\n",
    "    \"\"\"short price snapshot; prefer Finnhub → fallback yfinance (history if fast_info is missing)\"\"\"\n",
    "    symbol = symbol.upper().strip()\n",
    "\n",
    "    # try Finnhub first (clean fields, real-time-ish)\n",
    "    if _finnhub_client:\n",
    "        try:\n",
    "            q = _finnhub_client.quote(symbol)\n",
    "            c, pc, h, l, o = q.get(\"c\"), q.get(\"pc\"), q.get(\"h\"), q.get(\"l\"), q.get(\"o\")\n",
    "            if c:\n",
    "                chg = (None if not pc else round(100 * (c - pc) / pc, 2))\n",
    "                parts = [f\"**{symbol} (Finnhub)**\",\n",
    "                         f\"- Price: {c}\", f\"- Prev close: {pc}\",\n",
    "                         f\"- Open: {o}\", f\"- High: {h}\", f\"- Low: {l}\"]\n",
    "                if chg is not None: parts.append(f\"- Change: {chg}% vs prev close\")\n",
    "                return \"\\n\".join(parts)\n",
    "        except Exception:\n",
    "            pass  # fall through to yfinance\n",
    "\n",
    "    # yfinance: try fast_info, then compute from recent history if needed\n",
    "    try:\n",
    "        t = yf.Ticker(symbol)\n",
    "        fi = getattr(t, \"fast_info\", {}) or {}\n",
    "        c = fi.get(\"last_price\")\n",
    "        pc = fi.get(\"previous_close\")\n",
    "        h  = fi.get(\"day_high\")\n",
    "        l  = fi.get(\"day_low\")\n",
    "\n",
    "        # some tickers return None in fast_info — compute from history instead\n",
    "        if c is None or pc is None:\n",
    "            hist = t.history(period=\"5d\", interval=\"1d\")\n",
    "            if not hist.empty:\n",
    "                c = float(hist[\"Close\"].iloc[-1])\n",
    "                pc = float(hist[\"Close\"].iloc[-2]) if len(hist) > 1 else None\n",
    "                h = float(hist[\"High\"].iloc[-1])\n",
    "                l = float(hist[\"Low\"].iloc[-1])\n",
    "\n",
    "        if c is None:\n",
    "            return f\"Could not fetch price for {symbol}.\"\n",
    "\n",
    "        chg = (None if not pc else round(100 * (c - pc) / pc, 2))\n",
    "        parts = [f\"**{symbol} (yfinance)**\",\n",
    "                 f\"- Price: {c}\", f\"- Prev close: {pc}\",\n",
    "                 f\"- High: {h}\", f\"- Low: {l}\"]\n",
    "        if chg is not None: parts.append(f\"- Change: {chg}% vs prev close\")\n",
    "        return \"\\n\".join(parts)\n",
    "    except Exception as e:\n",
    "        return f\"Could not fetch price for {symbol}: {e}\"\n",
    "\n",
    "\n",
    "# ---------- DAILY SERIES ----------\n",
    "@tool\n",
    "def get_daily_series(symbol: str, days: int = 30) -> str:\n",
    "    \"\"\"quick trend summary (last N closes); prefer Alpha Vantage → fallback yfinance\"\"\"\n",
    "    symbol = symbol.upper().strip()\n",
    "    days = max(5, min(int(days), 200))  # keep it sane for class runs\n",
    "\n",
    "    # Alpha Vantage path\n",
    "    if _av_ok:\n",
    "        try:\n",
    "            ts = TimeSeries(key=os.getenv(\"ALPHAVANTAGE_API_KEY\"), output_format=\"pandas\")\n",
    "            data, _ = ts.get_daily(symbol=symbol, outputsize=\"compact\")\n",
    "            closes = data[\"4. close\"].tail(days)\n",
    "            if closes.empty:\n",
    "                return f\"No data from Alpha Vantage for {symbol}.\"\n",
    "            last_, first_ = float(closes.iloc[-1]), float(closes.iloc[0])\n",
    "            chg = round(100 * (last_ - first_) / first_, 2) if first_ else None\n",
    "            return (\n",
    "                f\"**{symbol} daily (Alpha Vantage, {len(closes)} days)**\\n\"\n",
    "                f\"- Last close: {last_}\\n- First close: {first_}\\n\"\n",
    "                + (f\"- Change: {chg}%\\n\" if chg is not None else \"\")\n",
    "                + f\"- Mean: {round(closes.mean(), 2)} | Std: {round(closes.std(), 2)}\"\n",
    "            )\n",
    "        except Exception:\n",
    "            pass  # fall through\n",
    "\n",
    "    # yfinance fallback\n",
    "    try:\n",
    "        end = datetime.utcnow().date()\n",
    "        start = end - timedelta(days=int(days * 2))  # buffer weekends/holidays\n",
    "        hist = yf.download(symbol, start=start, end=end, progress=False, auto_adjust=False)\n",
    "        closes = hist[\"Close\"].tail(days)\n",
    "        if closes.empty:\n",
    "            return f\"No recent daily data for {symbol}.\"\n",
    "        last_, first_ = float(closes.iloc[-1]), float(closes.iloc[0])\n",
    "        chg = round(100 * (last_ - first_) / first_, 2) if first_ else None\n",
    "        return (\n",
    "            f\"**{symbol} daily (yfinance, {len(closes)} days)**\\n\"\n",
    "            f\"- Last close: {last_}\\n- First close: {first_}\\n\"\n",
    "            + (f\"- Change: {chg}%\\n\" if chg is not None else \"\")\n",
    "            + f\"- Mean: {round(closes.mean(), 2)} | Std: {round(closes.std(), 2)}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"Could not fetch daily series for {symbol}: {e}\"\n",
    "\n",
    "\n",
    "# ---------- NEWS ----------\n",
    "@tool\n",
    "def get_recent_news(symbol: str, limit: int = 12) -> str:\n",
    "    \"\"\"recent headlines (publisher — title). Prefer Finnhub company_news; fallback to yfinance.\"\"\"\n",
    "    limit = max(1, min(int(limit), 25))\n",
    "    symbol = symbol.upper().strip()\n",
    "\n",
    "    # Finnhub company news (last ~14 days)\n",
    "    if _finnhub_client:\n",
    "        try:\n",
    "            from datetime import date, timedelta\n",
    "            end = date.today()\n",
    "            start = end - timedelta(days=14)\n",
    "            items = _finnhub_client.company_news(symbol, _from=start.isoformat(), to=end.isoformat()) or []\n",
    "            if items:\n",
    "                lines = []\n",
    "                for x in items[:limit]:\n",
    "                    src = x.get(\"source\") or \"Finnhub\"\n",
    "                    head = (x.get(\"headline\") or \"\").strip()\n",
    "                    url = x.get(\"url\") or \"\"\n",
    "                    if head:\n",
    "                        lines.append(f\"- **{src}** — {head} ({url})\")\n",
    "                if lines:\n",
    "                    return f\"**Recent news for {symbol}**\\n\" + \"\\n\".join(lines)\n",
    "        except Exception:\n",
    "            pass  # fall through\n",
    "\n",
    "    # yfinance fallback\n",
    "    try:\n",
    "        t = yf.Ticker(symbol)\n",
    "        raw = (t.news or [])[:limit]\n",
    "        if not raw:\n",
    "            return f\"No recent news for {symbol}.\"\n",
    "        lines = []\n",
    "        for x in raw:\n",
    "            pub = x.get(\"publisher\", \"Unknown\")\n",
    "            title = (x.get(\"title\") or \"\").strip()\n",
    "            link = x.get(\"link\", \"\")\n",
    "            lines.append(f\"- **{pub}** — {title} ({link})\")\n",
    "        return f\"**Recent news for {symbol}**\\n\" + \"\\n\".join(lines)\n",
    "    except Exception as e:\n",
    "        return f\"News fetch failed for {symbol}: {e}\"\n",
    "\n",
    "\n",
    "# ---------- NEWS ANALYSIS (optional chain) ----------\n",
    "@tool\n",
    "def analyze_news_report(symbol: str, limit: int = 12) -> str:\n",
    "    \"\"\"run full news→NLP chain if present; else show headlines with a tip\"\"\"\n",
    "    try:\n",
    "        _an = globals().get(\"analyze_news\")\n",
    "        _opt = globals().get(\"optimize_news\")\n",
    "        if callable(_an):\n",
    "            res = _opt(symbol, _an(symbol, limit=limit)) if callable(_opt) else _an(symbol, limit=limit)\n",
    "            rep = res.get(\"report\", {})\n",
    "            pct = rep.get(\"sentiment_pct\", {})\n",
    "            ents = rep.get(\"top_entities\", {})\n",
    "            n = rep.get(\"n_articles\", 0)\n",
    "            score = res.get(\"evaluator\", {}).get(\"score\")\n",
    "\n",
    "            out = [f\"**News analysis for {symbol.upper()}** (n={n})\",\n",
    "                   f\"- Sentiment % → pos {pct.get('positive',0)} | neu {pct.get('neutral',0)} | neg {pct.get('negative',0)}\"]\n",
    "            if ents:\n",
    "                chunks = []\n",
    "                for lbl, names in list(ents.items())[:3]:\n",
    "                    if names:\n",
    "                        chunks.append(f\"{lbl}: \" + \", \".join(names[:3]))\n",
    "                if chunks:\n",
    "                    out.append(\"- Top entities → \" + \" | \".join(chunks))\n",
    "            if score is not None:\n",
    "                out.append(f\"- Evaluator score → {score}/4\")\n",
    "            return \"\\n\".join(out)\n",
    "\n",
    "        # fallback path: just show headlines\n",
    "        return get_recent_news.invoke({\"symbol\": symbol, \"limit\": limit}) + \\\n",
    "               \"\\n\\n_(Tip: run the Prompt Chaining cell to enable full analysis.)_\"\n",
    "    except Exception as e:\n",
    "        return f\"News analysis failed for {symbol}: {e}\"\n",
    "\n",
    "\n",
    "# expose tool list for the agent build step\n",
    "TOOLS_EXTRA = [get_latest_price, get_daily_series, get_recent_news, analyze_news_report]\n",
    "print(\"🧰 Tools ready:\", [t.name for t in TOOLS_EXTRA])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Agent built using argument: prompt\n"
     ]
    }
   ],
   "source": [
    "#Cell 8\n",
    "#create all necessary tools for agent/steps of project:\n",
    "\n",
    "# ============================================================\n",
    "# Build agent (compatible with different langgraph versions)\n",
    "# ============================================================\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an Investment Research Assistant. Use tools when helpful.\\n\"\n",
    "    \"- Headlines/summaries → get_recent_news / analyze_news_report\\n\"\n",
    "    \"- Current price → get_latest_price\\n\"\n",
    "    \"- Short trend → get_daily_series\\n\"\n",
    "    \"Cite publishers by name; keep answers concise.\"\n",
    ")\n",
    "\n",
    "# make sure these exist\n",
    "memory = globals().get(\"memory\") or MemorySaver()\n",
    "tools  = [search] + TOOLS_EXTRA\n",
    "\n",
    "def build_agent():\n",
    "    # try common kwarg names across langgraph versions\n",
    "    for kw in (\"prompt\", \"state_modifier\", \"messages_modifier\"):\n",
    "        try:\n",
    "            a = create_react_agent(model=llm, tools=tools, checkpointer=memory,\n",
    "                                   **{kw: SystemMessage(SYSTEM_PROMPT)})\n",
    "            print(f\"🧠 Agent built using argument: {kw}\")\n",
    "            return a\n",
    "        except TypeError:\n",
    "            continue\n",
    "    # fallback: build without a system prompt; we can inject it at call time\n",
    "    a = create_react_agent(model=llm, tools=tools, checkpointer=memory)\n",
    "    print(\"🧠 Agent built without a system prompt (will inject at runtime).\")\n",
    "    return a\n",
    "\n",
    "agent = build_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "— Give me the current price for AAPL. —\n",
      "\n",
      "The current price for AAPL is $247.66, with a 0.97% increase from the previous close of $245.27. The stock opened at $249.38, with a high of $249.69 and a low of $245.56 today (Finnhub).\n",
      "\n",
      "— Show the last 20 days trend for MSFT. —\n",
      "\n",
      "The last 20 days trend for MSFT shows a decline of 5.76%, with the last close at $452.57 and the first close at $480.24. The average price over this period was $466.46 with a standard deviation of $9.38 (Alpha Vantage).\n",
      "\n",
      "— Summarize recent headlines for NVDA in 4 bullets. —\n",
      "\n",
      "- **Yahoo** — Morgan Stanley reaffirms Nvidia as a buy due to AI opportunities.\n",
      "- **Yahoo** — Broadcom stock rises with a new deal involving OpenAI chips.\n",
      "- **Yahoo** — Intel's fundamentals remain challenged despite a recent stock rally, according to BofA.\n",
      "- **Yahoo** — US leads in crypto investment with a significant ETF bet on Bitcoin.\n",
      "\n",
      "— Memory demo —\n",
      "\n",
      "Your watchlist includes AAPL, MSFT, and NVDA. The current price for AAPL is $247.66 (Finnhub).\n"
     ]
    }
   ],
   "source": [
    "#Cell 9 \n",
    "#smoke test with memory (thread_id is required when using a checkpointer)\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "THREAD = \"demo-1\"  # reuse this to persist memory across turns\n",
    "\n",
    "def ask(msg, thread=THREAD):\n",
    "    return agent.invoke(\n",
    "        {\"messages\": [SystemMessage(SYSTEM_PROMPT), (\"user\", msg)]},\n",
    "        config={\"configurable\": {\"thread_id\": thread}},\n",
    "    )\n",
    "\n",
    "# 1) Tool routing checks\n",
    "tests = [\n",
    "    \"Give me the current price for AAPL.\",\n",
    "    \"Show the last 20 days trend for MSFT.\",\n",
    "    \"Summarize recent headlines for NVDA in 4 bullets.\"\n",
    "]\n",
    "for msg in tests:\n",
    "    out = ask(msg)\n",
    "    print(\"\\n—\", msg, \"—\\n\")\n",
    "    print(out[\"messages\"][-1].content)\n",
    "\n",
    "# 2) Memory demo for the rubric (same thread_id)\n",
    "_ = ask(\"Remember that my watchlist is AAPL, MSFT, NVDA.\")  # stores preference in the thread\n",
    "out = ask(\"What's on my watchlist and current price for the first one?\")\n",
    "print(\"\\n— Memory demo —\\n\")\n",
    "print(out[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/jd/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 10 — NLP setup + helper functions for news analysis\n",
    "# - Preprocess → Sentiment (VADER) → NER (HF pipeline)\n",
    "# Rubric: Prompt-chaining/NLP pipeline\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# 1) NLTK: VADER for headline/summary sentiment\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find(\"sentiment/vader_lexicon\")\n",
    "except LookupError:\n",
    "    nltk.download(\"vader_lexicon\")\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "_vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# 2) HF Transformers: NER pipeline (small, fast-ish)\n",
    "from transformers import pipeline\n",
    "try:\n",
    "    _ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
    "except Exception:\n",
    "    # fallback in case torch/CPU hiccups — tiny model\n",
    "    _ner = pipeline(\"ner\", model=\"dslim/distilbert-NER\", aggregation_strategy=\"simple\")\n",
    "\n",
    "# ------------ utilities ------------\n",
    "def _clean_text(s: str) -> str:\n",
    "    \"\"\"light cleanup so NER/sentiment behaves (keep it simple for class speed).\"\"\"\n",
    "    s = (s or \"\").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def preproc_news(items: list[dict]) -> list[dict]:\n",
    "    \"\"\"normalize fields and add 'text_for_nlp'\"\"\"\n",
    "    out = []\n",
    "    for it in items or []:\n",
    "        head = _clean_text(it.get(\"title\") or it.get(\"headline\") or \"\")\n",
    "        summ = _clean_text(it.get(\"summary\") or \"\")\n",
    "        txt  = (head + \". \" + summ).strip(\". \")\n",
    "        out.append({**it, \"headline\": head, \"summary\": summ, \"text_for_nlp\": txt})\n",
    "    return out\n",
    "\n",
    "def classify_news(items: list[dict]) -> list[dict]:\n",
    "    \"\"\"add sentiment label + score using VADER compound\"\"\"\n",
    "    out = []\n",
    "    for it in items:\n",
    "        txt = it.get(\"text_for_nlp\", \"\")\n",
    "        sc  = _vader.polarity_scores(txt).get(\"compound\", 0.0)\n",
    "        if   sc >=  0.05: label = \"positive\"\n",
    "        elif sc <= -0.05: label = \"negative\"\n",
    "        else:             label = \"neutral\"\n",
    "        out.append({**it, \"sentiment\": label, \"sentiment_score\": round(sc, 3)})\n",
    "    return out\n",
    "\n",
    "def extract_entities(items: list[dict]) -> list[dict]:\n",
    "    \"\"\"add 'entities': list of {'text','label'} using HF NER\"\"\"\n",
    "    out = []\n",
    "    for it in items:\n",
    "        txt = it.get(\"text_for_nlp\", \"\")\n",
    "        ents = []\n",
    "        if txt:\n",
    "            try:\n",
    "                for e in _ner(txt):\n",
    "                    label = e.get(\"entity_group\") or e.get(\"entity\") or \"MISC\"\n",
    "                    word  = (e.get(\"word\") or e.get(\"text\") or \"\").replace(\"##\", \"\")\n",
    "                    if word.strip():\n",
    "                        ents.append({\"text\": word.strip(), \"label\": label})\n",
    "            except Exception:\n",
    "                pass\n",
    "        out.append({**it, \"entities\": ents})\n",
    "    return out\n",
    "\n",
    "# convenience: fetch recent news via yfinance for a symbol\n",
    "def fetch_news(symbol: str, limit: int = 12) -> list[dict]:\n",
    "    import yfinance as yf\n",
    "    symbol = symbol.upper().strip()\n",
    "    try:\n",
    "        t = yf.Ticker(symbol)\n",
    "        raw = (t.news or [])[:max(1, min(int(limit), 25))]\n",
    "        items = []\n",
    "        for x in raw:\n",
    "            items.append({\n",
    "                \"symbol\": symbol,\n",
    "                \"title\": x.get(\"title\"),\n",
    "                \"summary\": \"\",  # yfinance often lacks summaries\n",
    "                \"publisher\": x.get(\"publisher\"),\n",
    "                \"link\": x.get(\"link\"),\n",
    "                \"time_published\": x.get(\"providerPublishTime\"),  # unix seconds (may be None)\n",
    "            })\n",
    "        return items\n",
    "    except Exception:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 11\n",
    "#============================================================\n",
    "#Analyze_news() orchestration + optional evaluator\n",
    "#integrates: fetch → preprocess → sentiment → NER → aggregates\n",
    "#============================================================\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def analyze_news(symbol: str, limit: int = 12) -> dict:\n",
    "    \"\"\"\n",
    "    Orchestrates the news NLP pipeline for a ticker.\n",
    "    Steps: fetch recent items → preprocess → sentiment (VADER) → NER (HF) → aggregates.\n",
    "    Returns:\n",
    "        {\n",
    "          \"items\": [ ... per-article dicts incl. sentiment/entities ... ],\n",
    "          \"report\": {\n",
    "             \"symbol\": \"TICKER\",\n",
    "             \"n_articles\": int,\n",
    "             \"sentiment_pct\": {\"positive\": %, \"neutral\": %, \"negative\": %},\n",
    "             \"counts\": {\"positive\": n, \"neutral\": n, \"negative\": n},\n",
    "             \"top_entities\": {\"ORG\": [(name,count), ...], ...},\n",
    "             \"sample_headlines\": [str, ...]\n",
    "          }\n",
    "        }\n",
    "    \"\"\"\n",
    "    # be nice to graders / APIs\n",
    "    limit = max(1, min(int(limit), 25))\n",
    "\n",
    "    # 1) fetch\n",
    "    raw = fetch_news(symbol, limit=limit)\n",
    "\n",
    "    # 2) preprocess → sentiment → entities\n",
    "    pre  = preproc_news(raw)\n",
    "    cls  = classify_news(pre)\n",
    "    ner  = extract_entities(cls)\n",
    "\n",
    "    # 3) aggregates for report\n",
    "    n = len(ner)\n",
    "    \n",
    "    # early return if nothing to analyze\n",
    "    if n == 0:\n",
    "        return {\n",
    "            \"items\": [],\n",
    "            \"report\": {\n",
    "                \"symbol\": symbol.upper(),\n",
    "                \"n_articles\": 0,\n",
    "                \"sentiment_pct\": {\"positive\": 0.0, \"neutral\": 0.0, \"negative\": 0.0},\n",
    "                \"counts\": {\"positive\": 0, \"neutral\": 0, \"negative\": 0},\n",
    "                \"top_entities\": {},\n",
    "                \"sample_headlines\": []\n",
    "            }\n",
    "        }\n",
    "\n",
    "    counts = Counter([d.get(\"sentiment\", \"neutral\") for d in ner])\n",
    "    pct = {k: (round(100 * counts.get(k, 0) / n, 2) if n else 0.0)\n",
    "           for k in [\"positive\", \"neutral\", \"negative\"]}\n",
    "\n",
    "    # entities by type (top 5 each)\n",
    "    by_type = {}\n",
    "    for d in ner:\n",
    "        for e in d.get(\"entities\", []):\n",
    "            lbl = e.get(\"label\", \"MISC\")\n",
    "            txt = (e.get(\"text\") or \"\").strip()\n",
    "            if not txt:\n",
    "                continue\n",
    "            by_type.setdefault(lbl, Counter())[txt] += 1\n",
    "    top_entities = {lbl: by_type[lbl].most_common(5) for lbl in by_type}\n",
    "\n",
    "    report = {\n",
    "        \"symbol\": symbol.upper(),\n",
    "        \"n_articles\": n,\n",
    "        \"sentiment_pct\": pct,\n",
    "        \"counts\": dict(counts),\n",
    "        \"top_entities\": top_entities,\n",
    "        \"sample_headlines\": [d.get(\"headline\") for d in ner[:min(5, n)]],\n",
    "    }\n",
    "    return {\"items\": ner, \"report\": report}\n",
    "def optimize_news(symbol: str, analysis: dict) -> dict:\n",
    "    \"\"\"\n",
    "    OPTIONAL: quick LLM grader for clarity/coverage (0–4).\n",
    "    We keep it simple for class speed; ignore errors silently.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = (\n",
    "            \"You are grading a brief market news analysis for clarity and coverage.\\n\"\n",
    "            f\"Symbol: {symbol}\\n\"\n",
    "            f\"Report JSON: {analysis.get('report')}\\n\"\n",
    "            \"Return ONLY a number 0-4 where 4 is excellent.\"\n",
    "        )\n",
    "        resp = llm.invoke(prompt)\n",
    "        m = re.search(r\"\\b([0-4])\\b\", str(resp.content))\n",
    "        score = int(m.group(1)) if m else None\n",
    "    except Exception:\n",
    "        score = None\n",
    "    return {**analysis, \"evaluator\": {\"score\": score}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] n_articles: 8\n",
      "[Pipeline] sentiment %: {'positive': 62.5, 'neutral': 25.0, 'negative': 12.5}\n",
      "[Pipeline] top entities (sample): [('ORG', [('Broadcom', 3), ('OpenAI', 3), ('Intel', 2), ('AI', 2), ('Morgan Stanley', 1)]), ('MISC', [('AI', 4), ('co', 2), ('Am', 1), ('Deal', 1), ('American', 1)])]\n",
      "[Pipeline] evaluator score: 3\n",
      "\n",
      "[Agent] Tool-based summary:\n",
      "\n",
      "The news analysis for NVDA failed due to an error in processing the data. Here are the latest headlines for NVDA instead:\n",
      "\n",
      "- **Bloomberg**: NVIDIA Surges on AI Chip Demand\n",
      "- **Reuters**: NVIDIA Announces New GPU Lineup\n",
      "- **The Wall Street Journal**: NVIDIA's Market Cap Hits New High\n",
      "- **CNBC**: NVIDIA CEO Discusses Future of AI\n",
      "\n",
      "Would you like me to try the analysis again or do you have any other requests?\n"
     ]
    }
   ],
   "source": [
    "#Cell 12\n",
    "#============================================================\n",
    "#Quick checks for the news pipeline + agent tool\n",
    "#============================================================\n",
    "\n",
    "# --- A) Standalone pipeline sanity check (no agent) ---\n",
    "# goal: verify fetch → preprocess → sentiment → NER → aggregates works\n",
    "res = optimize_news(\"NVDA\", analyze_news(\"NVDA\", limit=8))\n",
    "print(\"[Pipeline] n_articles:\", res[\"report\"][\"n_articles\"])\n",
    "print(\"[Pipeline] sentiment %:\", res[\"report\"][\"sentiment_pct\"])\n",
    "print(\"[Pipeline] top entities (sample):\", list(res[\"report\"][\"top_entities\"].items())[:2])\n",
    "print(\"[Pipeline] evaluator score:\", res.get(\"evaluator\", {}).get(\"score\"))\n",
    "\n",
    "# --- B) Agent call that uses the tool (optional but great for rubric) ---\n",
    "# note: this uses the same thread_id so MemorySaver persists context\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "out = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            SystemMessage(SYSTEM_PROMPT),\n",
    "            (\"user\", \"Run a news analysis for NVDA and summarize the findings in 4 bullets.\")\n",
    "        ]\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": \"demo-1\"}},\n",
    ")\n",
    "print(\"\\n[Agent] Tool-based summary:\\n\")\n",
    "print(out[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Finnhub-pref] n_articles: 12\n",
      "[Finnhub-pref] sentiment %: {'positive': 58.33, 'neutral': 16.67, 'negative': 25.0}\n",
      "[Finnhub-pref] top entities (sample): [('ORG', [('Broadcom', 3), ('OpenAI', 3), ('Intel', 2), ('AI', 2), ('Morgan Stanley', 1)]), ('MISC', [('AI', 6), ('co', 3), ('Dow', 2), ('Am', 1), ('Deal', 1)])]\n",
      "[Finnhub-pref] evaluator score: 3\n"
     ]
    }
   ],
   "source": [
    "#Cell 13\n",
    "\n",
    "#============================================================\n",
    "#Optional: prefer Finnhub news (uses summary if present)\n",
    "#This strengthens sentiment/NER by giving more text to the pipeline.\n",
    "#============================================================\n",
    "\n",
    "def fetch_news(symbol: str, limit: int = 12) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Prefer Finnhub company_news (has 'summary'); fallback to yfinance.\n",
    "    Keeps the same output shape your pipeline expects.\n",
    "    \"\"\"\n",
    "    symbol = symbol.upper().strip()\n",
    "    limit = max(1, min(int(limit), 25))\n",
    "\n",
    "    items = []\n",
    "    used_finnhub = False\n",
    "\n",
    "    # 1) Finnhub path\n",
    "    try:\n",
    "        if '_finnhub_client' in globals() and _finnhub_client is not None:\n",
    "            from datetime import date, timedelta\n",
    "            end = date.today()\n",
    "            start = end - timedelta(days=14)\n",
    "            news = _finnhub_client.company_news(symbol, _from=start.isoformat(), to=end.isoformat()) or []\n",
    "            for n in news[:limit]:\n",
    "                items.append({\n",
    "                    \"symbol\": symbol,\n",
    "                    \"title\": n.get(\"headline\"),\n",
    "                    \"summary\": n.get(\"summary\") or \"\",\n",
    "                    \"publisher\": n.get(\"source\"),\n",
    "                    \"link\": n.get(\"url\"),\n",
    "                    \"time_published\": n.get(\"datetime\"),  # unix seconds\n",
    "                })\n",
    "            used_finnhub = len(items) > 0\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Fallback to yfinance if nothing from Finnhub\n",
    "    if not used_finnhub:\n",
    "        import yfinance as yf\n",
    "        try:\n",
    "            t = yf.Ticker(symbol)\n",
    "            raw = (t.news or [])[:limit]\n",
    "            for x in raw:\n",
    "                items.append({\n",
    "                    \"symbol\": symbol,\n",
    "                    \"title\": x.get(\"title\"),\n",
    "                    \"summary\": \"\",  # yfinance usually lacks summaries\n",
    "                    \"publisher\": x.get(\"publisher\"),\n",
    "                    \"link\": x.get(\"link\"),\n",
    "                    \"time_published\": x.get(\"providerPublishTime\"),\n",
    "                })\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return items\n",
    "\n",
    "# Quick re-check with richer text\n",
    "res = optimize_news(\"NVDA\", analyze_news(\"NVDA\", limit=12))\n",
    "print(\"[Finnhub-pref] n_articles:\", res[\"report\"][\"n_articles\"])\n",
    "print(\"[Finnhub-pref] sentiment %:\", res[\"report\"][\"sentiment_pct\"])\n",
    "print(\"[Finnhub-pref] top entities (sample):\", list(res[\"report\"][\"top_entities\"].items())[:2])\n",
    "print(\"[Finnhub-pref] evaluator score:\", res.get(\"evaluator\", {}).get(\"score\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The news analysis for NVDA failed again due to an error in processing the data. Here are the latest headlines for NVDA instead:\n",
      "\n",
      "- **Bloomberg**: NVIDIA Surges on AI Chip Demand\n",
      "- **Reuters**: NVIDIA Announces New GPU Lineup\n",
      "- **The Wall Street Journal**: NVIDIA's Market Cap Hits New High\n",
      "- **CNBC**: NVIDIA CEO Discusses Future of AI\n",
      "\n",
      "Would you like me to try the analysis again or do you have any other requests?\n"
     ]
    }
   ],
   "source": [
    "#Cell 14\n",
    "#============================================================\n",
    "#Agent call that exercises the news-analysis tool\n",
    "#WHAT this does:\n",
    "# - Sends the system instructions (how the agent should behave)\n",
    "# - Sends a user request (\"run a news analysis for NVDA...\")\n",
    "# - Uses a fixed thread_id so MemorySaver keeps context across turns\n",
    "# - Lets the agent choose tools (it should call analyze_news_report)\n",
    "# - Prints the assistant's final reply (last message content)\n",
    "# ============================================================\n",
    "\n",
    "from langchain_core.messages import SystemMessage  # already imported earlier; safe to re-import\n",
    "\n",
    "out = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            # System role: sets behavior & tool-usage guidance for this turn\n",
    "            SystemMessage(SYSTEM_PROMPT),\n",
    "\n",
    "            # User turn: our actual task for the agent\n",
    "            (\"user\", \"Run a news analysis for NVDA and summarize findings in 4 bullets.\")\n",
    "        ]\n",
    "    },\n",
    "    # Checkpointer config: using a stable thread_id keeps memory/state between runs\n",
    "    config={\"configurable\": {\"thread_id\": \"demo-1\"}},\n",
    ")\n",
    "\n",
    "# The agent returns a structured state that includes the conversation messages.\n",
    "# We print the assistant's final response text (last message in the list).\n",
    "print(out[\"messages\"][-1].content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Agent rebuilt using argument: prompt\n",
      "\n",
      "[Agent test]\n",
      " Here's a concise summary of the recent news analysis for NVDA:\n",
      "\n",
      "- **Market Sentiment**: Positive at 58.33%, Neutral at 16.67%, Negative at 25.0%.\n",
      "- **Key Entities**: Broadcom, OpenAI, and Intel are frequently mentioned alongside AI.\n",
      "- **Analyst Insights**: Morgan Stanley reaffirms NVDA as a Buy due to AI opportunities. Intel faces challenges despite a stock rally, per BofA.\n",
      "- **Broader Market**: The U.S. sees significant investment in Bitcoin ETFs.\n",
      "\n",
      "For more detailed insights, consider the following sources:\n",
      "- Morgan Stanley's reaffirmation from *Seeking Alpha*.\n",
      "- Broadcom's surge reported by *Bloomberg*.\n",
      "- Intel's downgrade covered by *TheStreet*.\n",
      "- Bitcoin ETF news from *CoinDesk*.\n"
     ]
    }
   ],
   "source": [
    "# Cell 15 — Wrapper tool so the agent passes args explicitly\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool(\"run_news_analysis\")\n",
    "def run_news_analysis(ticker: str, limit: int = 12, bullets: int = 4) -> str:\n",
    "    \"\"\"\n",
    "    Run the full news pipeline for `ticker` and return a bullet summary.\n",
    "    Args:\n",
    "      - ticker (str): e.g., \"NVDA\"\n",
    "      - limit (int): number of recent articles to analyze\n",
    "      - bullets (int): bullet count for the final summary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # use the same core functions the pipeline uses\n",
    "        res = optimize_news(ticker, analyze_news(ticker, limit=limit))\n",
    "        rep = res[\"report\"]\n",
    "        n   = rep.get(\"n_articles\", 0)\n",
    "        pct = rep.get(\"sentiment_pct\", {})\n",
    "        ents = rep.get(\"top_entities\", {})\n",
    "        score = res.get(\"evaluator\", {}).get(\"score\")\n",
    "\n",
    "        lines = [\n",
    "            f\"**News analysis for {rep['symbol']}** (n={n})\",\n",
    "            f\"- Sentiment: +{pct.get('positive',0)}% / {pct.get('neutral',0)}% / -{pct.get('negative',0)}%\",\n",
    "        ]\n",
    "        # compact entity preview\n",
    "        if ents:\n",
    "            preview = []\n",
    "            for lbl, pairs in list(ents.items())[:2]:\n",
    "                preview.append(f\"{lbl}: \" + \", \".join([f\"{name}({cnt})\" for name, cnt in pairs[:5]]))\n",
    "            lines.append(\"- Top entities: \" + \" | \".join(preview))\n",
    "        if score is not None:\n",
    "            lines.append(f\"- Evaluator score: {score}/4\")\n",
    "\n",
    "        # add sample headlines up to `bullets`\n",
    "        heads = [h for h in rep.get(\"sample_headlines\", []) if h][:bullets]\n",
    "        if heads:\n",
    "            lines.append(\"- Headlines:\")\n",
    "            for h in heads:\n",
    "                lines.append(f\"  • {h}\")\n",
    "\n",
    "        return \"\\n\".join(lines)\n",
    "    except Exception as e:\n",
    "        return f\"run_news_analysis failed: {e}\"\n",
    "\n",
    "# add the new tool and rebuild the agent\n",
    "TOOLS_EXTRA = [get_latest_price, get_daily_series, get_recent_news, analyze_news_report, run_news_analysis]\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an Investment Research Assistant. Use tools when helpful.\\n\"\n",
    "    \"- For a compact end-to-end NVDA-style report, call run_news_analysis(ticker, limit, bullets).\\n\"\n",
    "    \"- Headlines/summaries → get_recent_news / analyze_news_report (if a specific ticker is provided).\\n\"\n",
    "    \"- Current price → get_latest_price\\n\"\n",
    "    \"- Short trend → get_daily_series\\n\"\n",
    "    \"Always pass explicit arguments to tools. Cite publishers by name; keep answers concise.\"\n",
    ")\n",
    "\n",
    "memory = MemorySaver()\n",
    "tools  = [search] + TOOLS_EXTRA  # assumes `search` is already defined in Cell 5\n",
    "\n",
    "def build_agent():\n",
    "    # try common kwarg names across langgraph versions\n",
    "    for kw in (\"prompt\", \"state_modifier\", \"messages_modifier\"):\n",
    "        try:\n",
    "            a = create_react_agent(model=llm, tools=tools, checkpointer=memory,\n",
    "                                   **{kw: SystemMessage(SYSTEM_PROMPT)})\n",
    "            print(f\"🧠 Agent rebuilt using argument: {kw}\")\n",
    "            return a\n",
    "        except TypeError:\n",
    "            continue\n",
    "    a = create_react_agent(model=llm, tools=tools, checkpointer=memory)\n",
    "    print(\"🧠 Agent rebuilt (no system prompt at build; will inject at runtime).\")\n",
    "    return a\n",
    "\n",
    "agent = build_agent()\n",
    "\n",
    "# quick smoke test: force a call with explicit args in the user request\n",
    "out = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            SystemMessage(SYSTEM_PROMPT),\n",
    "            (\"user\", \"Use run_news_analysis with ticker=NVDA, limit=12, bullets=4.\")\n",
    "        ]\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": \"demo-1\"}},\n",
    ")\n",
    "print(\"\\n[Agent test]\\n\", out[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a concise summary of the recent news analysis for AAPL:\n",
      "\n",
      "- **Market Sentiment**: Positive at 70.0%, Neutral at 0.0%, Negative at 30.0%.\n",
      "- **Key Entities**: Apple is central, with mentions of JPMorgan and AI technologies like iPhone 17 and Apple Intelligence.\n",
      "- **Analyst Insights**: Apple's stock performance is debated; some suggest holding due to recent gains, while others see no positive catalysts until spring.\n",
      "- **Broader Market**: JPMorgan's significant investment in U.S. national security is noted.\n",
      "\n",
      "For more detailed insights, consider the following sources:\n",
      "- Apple's stock performance discussed by *Investor's Business Daily*.\n",
      "- Apple's future catalysts covered by *Motley Fool*.\n",
      "- JPMorgan's investment reported by *Reuters*.\n",
      "- Cash-producing stocks analysis from *TheStreet*.\n"
     ]
    }
   ],
   "source": [
    "#Cell 16 — Quick interactive demo\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "def demo(ticker=\"NVDA\", limit=12, bullets=4, thread=\"demo-1\"):\n",
    "    prompt = f\"Use run_news_analysis with ticker={ticker}, limit={limit}, bullets={bullets}.\"\n",
    "    out = agent.invoke(\n",
    "        {\"messages\": [SystemMessage(SYSTEM_PROMPT), (\"user\", prompt)]},\n",
    "        config={\"configurable\": {\"thread_id\": thread}},\n",
    "    )\n",
    "    print(out[\"messages\"][-1].content)\n",
    "\n",
    "#Try a few - (\"ticker code\", # of recent articles, # of bullets)\n",
    "#demo(\"NVDA\", 12, 4) #12 recent artivles, 4 bullets\n",
    "demo(\"AAPL\", 10, 4) \n",
    "#demo(\"MSFT\", 10, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pass 1] score=3 | sentiment %={'positive': 58.33, 'neutral': 16.67, 'negative': 25.0}\n",
      "[Refinement] Score acceptable — no retry.\n"
     ]
    }
   ],
   "source": [
    "#Cell 16a — Evaluator→Optimizer refinement demo\n",
    "#If the first pass scores low, re-run with more articles to improve coverage.\n",
    "\n",
    "ticker = \"NVDA\"\n",
    "\n",
    "res = optimize_news(ticker, analyze_news(ticker, limit=12))\n",
    "score = res.get(\"evaluator\", {}).get(\"score\")\n",
    "print(f\"[Pass 1] score={score} | sentiment %={res['report']['sentiment_pct']}\")\n",
    "\n",
    "if score is not None and score < 3:\n",
    "    print(\"[Refinement] Score is low — increasing article limit and retrying…\")\n",
    "    res = optimize_news(ticker, analyze_news(ticker, limit=20))\n",
    "    score = res.get(\"evaluator\", {}).get(\"score\")\n",
    "    print(f\"[Pass 2] score={score} | sentiment %={res['report']['sentiment_pct']}\")\n",
    "else:\n",
    "    print(\"[Refinement] Score acceptable — no retry.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cell 17 — Simple sentiment visualization for the rubric\n",
    "#Shows % positive/neutral/negative from the latest analysis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sym = \"NVDA\"\n",
    "res = optimize_news(sym, analyze_news(sym, limit=12))\n",
    "pct = res[\"report\"][\"sentiment_pct\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar([\"positive\", \"neutral\", \"negative\"],\n",
    "        [pct.get(\"positive\", 0), pct.get(\"neutral\", 0), pct.get(\"negative\", 0)])\n",
    "plt.title(f\"{sym} — Sentiment % (last ~12 news)\")\n",
    "plt.ylabel(\"%\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aai520-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
